# üìö TechChallenge - API para Consulta de Livros

![FastAPI](https://img.shields.io/badge/FastAPI-v2.1.0-green)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![Status](https://img.shields.io/badge/status-active-brightgreen)

---

## üîé Sobre o Projeto

Esta API, constru√≠da com **FastAPI**, realiza web scraping no site [Books to Scrape](http://books.toscrape.com/) para coletar dados de livros, armazen√°-los localmente em um arquivo `.csv` e disponibiliz√°-los via endpoints REST.

O objetivo principal √© demonstrar o fluxo completo de integra√ß√£o entre coleta de dados (scraping), manipula√ß√£o e armazenamento, e disponibiliza√ß√£o via API para consumo por cientistas de dados, desenvolvedores e demais interessados.

---

## ‚öôÔ∏è Funcionalidades

- üìñ Listar todos os livros ou filtr√°-los por categoria.
- üîç Buscar livros por t√≠tulo e/ou categoria.
- üè∑Ô∏è Listar todas as categorias dispon√≠veis.
- üÜî Consultar um livro pelo seu ID √∫nico.
- ü©∫ Verificar o status da API e conectividade com o banco de dados.
- üíæ Exportar dados completos para CSV (via gera√ß√£o autom√°tica no backend).

## ‚öôÔ∏è Requisitos

- Python 3.11+
- Git (opcional, para clonar o reposit√≥rio)
- VSCode

## üõ† Tecnologias Utilizadas

| Tecnologia      | Descri√ß√£o                              |
|-----------------|--------------------------------------|
| FastAPI         | Framework web moderno e r√°pido       |
| Requests        | Requisi√ß√µes HTTP para scraping       |
| BeautifulSoup4  | Extra√ß√£o de dados HTML                |
| Pandas          | Manipula√ß√£o e exporta√ß√£o de dados    |
| Uvicorn         | Servidor ASGI para execu√ß√£o da API   |
| Pydantic        | Valida√ß√£o de modelos e dados         |


---

## üì• Instala√ß√£o

### 1.Instalar o Python

### 2.Instalar o VSCode (ambiente sugerido)

### 3.Instalar o Git (opcional)


## üì¶ Instru√ß√µes de Configura√ß√£o e Execu√ß√£o

0. **Abre o VS Code**

1. **Obten√ß√£o do projeto**    
    **a.Clone o reposit√≥rio**
    ```bash
    git clone https://github.com/devMansano/TechChallenge.git
    cd TechChallenge
    ```
   **b.Download dos arquivos do Git e abertura da pasta no VSCode**
    

2. **Crie e ative um ambiente virtual**
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # Linux/Mac
    source venv/bin/activate

3. **Instale as depend√™ncias**
    pip install -r requirements.txt

4. **Execute o codigo Main.py**
    python Main.py

5. **No acesso Local, utilize: http://127.0.0.1:8000/ ou http://localhost:8000/ para acessar a API**

6. **Utilize os pontos de acesso ap√≥s o localhost**
    

## üìÅ Estrutura do Projeto

```plaintext
TechChallenge/
¬¶   README.md                   # Documenta√ß√£o do projeto (este arquivo)
¬¶   Main.py                     # Arquivo principal que inicia a API FastAPI
¬¶       
+---Config                      # Arquivos de configura√ß√£o
¬¶       render.yaml             # Arquivo de configura√ß√£o do render.com
¬¶       requirements.txt        # Depend√™ncias do projeto
¬¶                 
+---Dados                       # Dados coletados e processados
¬¶   ¬¶   books_complete.csv      # Banco de dados local gerado via scraping
¬¶   ¬¶   Extracao.py             # Fun√ß√µes para extrair categorias e livros do site
¬¶   ¬¶   gera_base.py            # Cria√ß√£o e leitura do CSV com os dados coletados
¬¶   
+---Modelo                      # Modelos Pydantic para valida√ß√£o de dados
¬¶   ¬¶   Livro.py                # Defini√ß√£o do modelo Book
¬¶   
¬¶   
+---Outros                      # Arquivos variados
¬¶       Diagrama.drawio         # Diagrama em Bloco para ser utilizado no site: https://www.drawio.com/
¬¶       Diagrama.jpeg           # Diagrama em Imagem
¬¶       Pos_tech - Tech Challenge - Fase 1 - Machine Learning Engineering.pdf           # Arquivo de apoio do pedido
¬¶       
+---Rota                        # Endpoints da API organizados em m√≥dulos
¬¶   ¬¶   bem_vindo.py            # Rota raiz com mensagem de boas-vindas
¬¶   ¬¶   conexao.py              # Endpoint para checar sa√∫de da API / status da base
¬¶   ¬¶   id.py #                 # Busca de livro pelo ID
¬¶   ¬¶   listar_categorias.py    # Rota para listar categorias dispon√≠veis
¬¶   ¬¶   listar_livros.py        # Rota para listagem geral de livros
¬¶   ¬¶   titulo_categoria.py     # Busca de livros por t√≠tulo e/ou categoria
```

## üìå Pontos de Acesso
M√©todo	Endpoint	Descri√ß√£o

- GET	/	--> Mensagem de boas-vindas
- GET	/categorias	--> Lista todas as categorias
- GET	/livros -->	Lista todos os livros (ou filtra por categoria)
- GET	/api/v1/id/{Id} -->	Busca livro por ID
- GET	/api/v1/busca?title=&category -->	Busca por t√≠tulo e/ou categoria
- GET	/api/v1/conexao -->	Verifica status da API

### Exemplos de retorno para cada rota da API

---

#### 1Ô∏è‚É£ Listar todos os livros
**Rota:** `/livros/api/v1/livros`  
**Exemplo de sa√≠da:**  
![Exemplo - Listar Livros](Exemplos/listar_livros.png)

---

#### 2Ô∏è‚É£ Buscar livro por ID
**Rota:** `/api/v1/id/234`  
**Exemplo de sa√≠da:**  
![Exemplo - Buscar por ID](Exemplos/id.png)

---

#### 3Ô∏è‚É£ Listar todas as categorias
**Rota:** `/api/v1/categorias/`  
**Exemplo de sa√≠da:**  
![Exemplo - Categorias](Exemplos/categorias.png)

---

#### 4Ô∏è‚É£ Buscar livro por t√≠tulo e/ou categoria
**Rota:** `/api/v1/busca/?titulo=Starlark`  
**Exemplo de sa√≠da (Titulo):**  
![Exemplo - Busca com Filtros](Exemplos/busca_titulo.png)  
**Rota:** `/api/v1/busca/?categoria=Travel`  
**Exemplo de sa√≠da (Categoria):**  
![Exemplo - Busca com Filtros](Exemplos/busca_categoria.png)  
**Rota:** `/api/v1/busca/?titulo=Starlark&categoria=Crime`  
**Exemplo de sa√≠da (Titulo e/ou Categoria):**  
![Exemplo - Busca com Filtros](Exemplos/busca_titulo_categoria.png)

---

#### 5Ô∏è‚É£ Testar conex√£o com a API
**Rota:** `/api/v1/conexao/`  
**Exemplo de sa√≠da:**  
![Exemplo - Conex√£o](Exemplos/conexao.png)


## üì§ Exportar para CSV
- O arquivo .CSV gerado √© armazenado dentro da pasta Dados com nome base_livros.csv.


## üìÑ Licen√ßa
- Este projeto √© apenas para fins educacionais.
O site Books to Scrape √© destinado a pr√°ticas de web scraping e n√£o cont√©m dados reais.